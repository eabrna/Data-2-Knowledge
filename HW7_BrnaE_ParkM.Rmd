---
title: "Homework 07"
author: "Brna, Elliot; Park, Mia" # <--- Please change to your names here, format: "LastName, FirstName"
date: 'Due: Wed Nov 6| 11:59pm' # <--- Please change date for each homework
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

**APMA 3150 \| Fall 2024 \| University of Virginia**

***"On my honor, I pledge that I have neither given nor received unauthorized aid on this assignment." - The author of this RMarkdown file.***

<!--- Solution Region --->

```{css solution-region, echo=FALSE}
.solution {
    background-color: #232D4B10;
    border-style: solid;
    border-color: #232D4B;
    padding: .5em;
    margin: 20px
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.


## [Problems]{style="color:#FF7F50"}

We need to first load Verzani's textbook packages `MASS`, `UsingR`, our course package `APMA3150` in our working environment. 
```{r message=FALSE}
library(MASS)
library(UsingR)
library(APMA3150)
```

### 1. Example 8.13 from URIS: Stress Test

A hard-drive manufacturer would like to ensure that the mean time between failures (MTBF) for its new hard drive is 1 million hours. A stress test
is designed that can simulate the workload at a much faster pace. The testers assume that a test lasting 10 days correlates with the failure time exceeding
the 1-million-hour mark. In stress tests of 15 hard drives they found an average of 9.5 days, with a standard deviation of 1 day. Does a 90% confidence level include 10 days?

#### {.solution}

```{r}
mu = 9.5
n = 15
s = 1
alpha = 0.1
t <- qt(1-alpha/2, df = n-1)
SE <- s/sqrt(n)

conf_interval <- mu + c(-1,1) * (t *SE)
conf_interval
```
The 90% confidence interval ([9.045232, 9.954768]) does not include 10 days. 

### 2. Example 8.22 from URIS: Confidence interval based on different probability distribution.

When the parent population is Normal ( μ, s ) with known s, then confidence intervals of the type
 ̄x ± z ⇤ SD (  ̄x ) and  ̄x ± t ⇤ SE (  ̄x )are both applicable. We have that far enough in the tail, z ⇤ < t ⇤ , but sometimes s < s, so there is no clear winner as to which confidence interval will be smaller for a given sample. Run a simulation 200 times in which the margin of error is calculated both ways for a sample of size 10 with s = 2 and μ = 0. Use a 90% confidence level. What percent of the time was the confidence interval using SD (  ̄x ) smaller?

#### {.solution}
```{r}
# Type your solution here...
n=10
alpha = .1

t = qt(1-alpha/2, df = n-1)
z = qnorm(1-alpha/2)

count = 0


for (i in 1:200) {
 
  # Generate the random sample
  sample <- rnorm(n, mean=0, sd = 2)
  x_bar <- mean(sample)
  s <- sd(sample)
  
  # Margin of errors calculations
  SD = 2/sqrt(n)
  SE = s/sqrt(n)
  
  ME_z = z*SD
  ME_t = t*SE
  
  # check if the z/SD based interval is smaller/add to count
  if (ME_z < ME_t) {
    count = count + 1
  }
  
}

percent_smaller <- (count/200)*100
cat("SD/z-based interval was smaller", percent_smaller, "% of the time")


```
Write your conclusion here...


### 3. Example 9.5 from URIS: Electronic toll-collection systems.

On a number of highways a toll is collected for permission to travel on the roadway. To lessen the burden on drivers, electronic toll-collection systems are often used. An engineer wishes to check the validity of one such system. She arranges to survey a collection unit for single day, finding that of 5,760 transactions, the system accurately read 5,731. Perform a one-sided significance test to see if this is consistent with a 99.9% accuracy rating at the 0.05 significance level. (Do you have any doubts that the normal approximation to the binomial distribution should apply here?)

#### {.solution}
```{r}
x = 5731
n = 5760
p = 0.999

prop.test(x, n, p, alternative = 'greater', conf.level = 0.95, correct = FALSE)
```
The p-value (1) of this test is much higher than that of 0.05 indicating that we fail to reject the null hypothesis. There is not statistical evidence that the electronic toll system's accuracy is consistent with a 99.9% accuracy rating.

There are doubts about using a normal approximation here. For the normal approximation to binomial distribution to be reliable, both np and n(1-p) should be greater than 5. The value of n(1-p) is around 5.76 which is very close to 5 and could be the cause of the high p-value seen in this test.


### 4. Example 9.6 from URIS: Poverty rate

In an example on the poverty rate a count of 22,695 in a survey of 150,000 produced a p-value of 0.079. For the same size sample, what range of counts would have produced a p-value less than 0.05? (Start by asking what observed proportions in the survey would have such a p-value.)

#### {.solution}
```{r}
# Type your solution here...
n = 150000
p.hat = 22695/150000
SD = sqrt(p.hat*(1-p.hat)/n)

# critical z-score for alpha = .05 (.025 bc two tailed test)
z_crit = qnorm(1-.05/2)

# range of proportions that give p < .05
p.lower = p.hat - z_crit*SD
p.upper = p.hat + z_crit*SD

# counts from proportions - round down for upper and up for lower
low_count = ceiling(p.lower*n)
high_count = floor(p.upper*n)

cat("The range of counts is (", low_count, ",", high_count, ")")



```
Write your conclusion here...


### 5. Example 9.11 from URIS: Father's height

In the babies (UsingR) data set, the variable dht records the father’s height for the sampled cases. Do a significance test of the null hypothesis that the population mean height is 68 inches against an alternative that it is taller. Remove the values of 99 from the data, as these indicate missing data.
#### {.solution}
```{r}
X = 68
babies_updated <- subset(babies, dht != 99)
dht <- babies_updated$dht

t_test_result <- t.test(dht, mu = X, alternative = "greater")
t_test_result
```
The p-value from this test is extremely small, indicating that we can reject the null hypothesis at a 95% confidence interval. This shows that there is statistical evidence that population mean height for fathers is greater than 68 inches. 

### 6. Example 9.16 from URIS: Find sample size

A one-sided, one-sample t-test will be performed. What sample size is needed to have a power of 0.80 for a significance level of 0.05 if delta=.5 and the population standard deviation is assumed to be 5?

Note: please use power.t.test()

#### {.solution}
```{r}
# Type your solution here...
s = 5
sig = .05
pwr = .8
size = power.t.test(delta = .5, power = pwr, sd=s, sig.level=sig, type = "one.sample", alternative = "one.sided" )$n

```
Write your conclusion here...
A sample size of ~620 is needed to reach a power level of 0.8 with a significance of .05


### 7. Coin tosses

Let 𝑋 = number of heads in n coin tosses (assume the coin is fair). Let 𝑌 = 𝑋 – 𝑛/2.

(i) Plot (on the same graph) 20 randomly generated histories of 𝑌 vs. 𝑛 up to 𝑛 = 1000 (the
plot should look like the matplot() with 1000 games in roulette case study).
(ii) Superimpose on this plot the line of 𝐸(𝑌) versus 𝑛 and the curves for 𝐸(𝑌) ± 2/𝑉𝑎𝑟(𝑌)
versus 𝑛.
(iii) Next, let 𝑝̂ = 𝑋/𝑛, and plot the 20 simulated histories of 𝑝̂ versus 𝑛, together with the
lines of 𝐸(𝑝̂ ) versus n and the curves of 𝐸(𝑝̂ ) ± 2/𝑉𝑎𝑟(𝑝̂ ) versus 𝑛.
(iv) Discuss your plots.

#### {.solution}
```{r}
# (i)
cum.Y <- matrix(NA, nrow=1000, ncol=20)

for (i in 1:20) {
  X <- cumsum(rbinom(1000, 1, 0.5))
  Y <- X - (1:1000) / 2
  cum.Y[, i] <- Y
}

cum.Y <- rbind(0, cum.Y)

matplot(0:1000, cum.Y, type="l", xlab="Number of tosses (n)", ylab="Y", col = rainbow(20))
abline(h=0)

# (ii)
n_seq <- 0:1000
EY <- rep(0, length(n_seq)) # E(Y) is zero for all n
SDY <- sqrt(n_seq / 4)

lines(n_seq, EY, col = "black",lwd = 5)          
lines(n_seq, EY + 2 * SDY, col = "black", lwd = 2)   
lines(n_seq, EY - 2 * SDY, col = "black", lwd = 2)

# (iii)
cum.phat <- matrix(NA, nrow=1000, ncol=20)
for (i in 1:20) {
  X <- cumsum(rbinom(1000, 1, 0.5))
  p_hat <- X / (1:1000)
  cum.phat[, i] <- p_hat
}

cum.phat <- rbind(0, cum.phat)
matplot(0:1000, cum.phat, type="l", xlab="Estimated Porpotion of Heads(p_hat)", ylab="Y", col = rainbow(20))
abline(h=0.5)

n_seq <- 0:1000
Ephat <- rep(0.5, length(n_seq)) # E(Y) is zero for all n
SDphat <- 1 / (2 * sqrt(n_seq)) 

lines(n_seq, Ephat, col = "black",lwd = 5)          
lines(n_seq, Ephat + 2 * SDphat, col = "black", lwd = 2)   
lines(n_seq, Ephat - 2 * SDphat, col = "black", lwd = 2)
```
(iv). The plots show that as the number of coin tosses increases, both the cumulative deviation (Y) and the estimated proportion of heads (p_hat) behave as expected for a fair coin. The deviation in Y remains within the expected bounds, and the estimated proportion stabilizes around 0.5, with decreasing variability. This confirms that larger sample sizes lead to more accurate and stable estimates of the true probability. 


### 8. Squared relationship

Let 𝑋 be a standard normal random variable. Let 𝑌 = 𝑋 ! .

(i) Write R code to plot the probability density function (PDF) of 𝑋 for values in the range (-
3,3) with points separated by 0.1. Note: there should be 61 values in the specified range.
(ii) Write R code to determine the PDF of 𝑌 using equation in Item 34 of stat-techniques.pdf
for the same range of 𝑋 values as in Part (i). Plot this PDF.
(iii) What is the distribution of 𝑌? Find density function values using the built-in R function
for this distribution for the same range of 𝑌 as in Part (ii). Plot these computed density
function values against the PDF values computed in Part (ii). (Hint: see Item 28 of stat-
techniques.pdf)
(iv) Discuss the plot of part (iii).


#### {.solution}
```{r}
# Type your solution here...

# (i) PDF of standard normal variable X from -3:3
X = seq(from = -3, to = 3, by =.1)
pdf_x = dnorm(X)
plot(X, pdf_x,  ylab = "PDF of X" , xlab = "X")

# (ii) pdf of Y from Item 34 from stat-techniques
Y = range^2
pdf_y1 = 1:61

pdf_y1 <- (1 / (2 * sqrt(Y))) * (dnorm(sqrt(Y)) + dnorm(-sqrt(Y)))

#plot
plot(Y, pdf_y1, ylab = "PDF of Y", xlab = "Y")


# (iii) PDF for Y according to item 28 of stat-techniques
pdf_y2 <- dchisq(Y, df = 1)

# plotting both
plot(Y, pdf_y1, ylab = "PDF of Y", xlab = "Y")
points(Y, pdf_y2, pch = 3, col = "red")



```
Write your conclusion here...
(iv) Both of the PDF plots are the same, hence the overlap in shape


### 9. Winning in the Roulette game

Generate 20 histories with 1000 runs each assuming a 0.5 probability of winning in the roulette
game (black-red bet). Plot the matplot() of 20 histories with 1000 runs each with the three
additional lines for mean and 95% confidence interval (as we did in the Roulette case study).
Discuss your plots.


#### {.solution}
```{r}
cum.phat <- matrix(NA, nrow=1000, ncol=20)
for (i in 1:20) {
  X <- cumsum(rbinom(1000, 1, 0.5))
  p_hat <- X / (1:1000)
  cum.phat[, i] <- p_hat
}

cum.phat <- rbind(0, cum.phat)
matplot(0:1000, cum.phat, type="l", xlab="Estimated Porpotion of Heads(p_hat)", ylab="Y", col = rainbow(20))
abline(h=0.5)

n_seq <- 0:1000
Ephat <- rep(0.5, length(n_seq))
SDphat <- sqrt(0.5 * 0.5 / n_seq) 

lines(n_seq, Ephat, col = "black",lwd = 5)          
lines(n_seq, Ephat + 1.96 * SDphat, col = "black", lwd = 2)   
lines(n_seq, Ephat - 1.96 * SDphat, col = "black", lwd = 2)
```
The plot displays the estimated proportions of wins from 20 simulated histories of a roulette game, each consisting of 1000 runs with a 0.5 probability of winning. As expected, the average estimated proportion remains stable around the true probability, while the shaded region representing the 95% confidence intervals show how the uncertainty decreases with more trials. Overall, the variability among 20 histories reflects the randomness of the game, but they all converge towards the expected mean as the number of trials grows.

### 10. Inter-arrival times

Call inter-arrival times are exponentially distributed with an arrival rate of 2/sec. Using 100
arrivals, draw a sample path showing number of calls as a function of time. Discretize time in 0.1
sec intervals.

(i) Include the graph.
(ii) What is the mean number of calls in the system at 30 sec?
(iii) What is the mean of the time taken for 100 arrivals?


#### {.solution}
```{r}
# Type your solution here...
lamda = 2
n = 100
int = 0.1

call_times <- rexp(n, rate = lamda)

time = seq(0, 9.9, by = .1)
sample.path = cumsum(call_times)
plot(time, sample.path, xlab="Time (s)", ylab = "Number of calls")

mean_30s = 30*lamda
cat("Mean number of calls in 30 seconds:", mean_30s, "\n")

mean_100calls = 100/lamda
cat("Mean time to reach 100 call inter-arrivals", mean_100calls)


```
Write your conclusion here...
