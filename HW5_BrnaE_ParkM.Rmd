---
title: "Homework 05"
author: "Brna, Elliot; Park, Mia" # <--- Please change to your names here, format: "LastName, FirstName"
date: 'Due: Wed Oct 23 | 11:59pm' # <--- Please change date for each homework
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

**APMA 3150 \| Fall 2024 \| University of Virginia**

***"On my honor, I pledge that I have neither given nor received unauthorized aid on this assignment." - The author of this RMarkdown file.***

<!--- Solution Region --->

```{css solution-region, echo=FALSE}
.solution {
    background-color: #232D4B10;
    border-style: solid;
    border-color: #232D4B;
    padding: .5em;
    margin: 20px
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.

## [Problems]{style="color:#FF7F50"}

We need to first load Verzani's textbook packages `MASS`, `UsingR`, our course package `APMA3150` in our working environment. 
```{r message=FALSE}
library(MASS)
library(UsingR)
library(APMA3150)
```

### 1. Plant a 10-fold cross-validation tree. (Boston dataset)
Use a random forest model to predict medv, the median value of owner-occupied homes (in
$1000s). Use the default parameters and report your 10-fold cross-validation MSE.

Hint: you may use a for-loop to train 10 tree models based on the training/testing data splits.
Note: please use random seed number 3, and use sample() function for setting the folds randomly.

#### {.solution}

```{r}
set.seed(3)

mse_values = numeric(10)

for (i in 1:10) {
  set.seed(sample(1:1000, 1))
  train = sample(1:nrow(Boston), nrow(Boston)/2)
  boston.test = Boston[-train, "medv"]
  rf.boston = randomForest(medv ~ ., data = Boston, subset = train, importance = TRUE)
  yhat.rf = predict(rf.boston, newdata = Boston[-train, ])
  mse_values[i] = mean((yhat.rf - boston.test)^2)
}

plot(yhat.rf, boston.test)
abline(0, 1, col = 'red')
mse_values
```


### 2. Best random forest tree. (Boston dataset)
Now we will vary the tuning parameters of mtry and ntree to see what effect they have on
performance

Use a range of reasonable mtry and ntree values.

• Run random forest 5 times for each tuning set (each time the training data comes from
sample(nrow(Boston), replace = TRUE); this is called bootstrapping), find
the out-of-bag MSE each time (out-of-bag data is not used to train the model, so it is a
good test for the performance of the model) and use the average for the out-of-bag MSEs
associated with the tuning parameters.
• When you use the randomForest package, the mse element in the output is a vector
of out-of-bag MSE values for 1:ntree trees in the forest. This means that you can
set ntree to some maximum value and get the out-of-bag MSE for any number of trees
less than or equal to ntree.
• Make a plot to show the average out-of-bag MSE as a function of mtry and ntree.
• Report the best tuning parameter combination of mtry and ntree.

Note: random forest is a stochastic model; it will be different every time it runs. Set your own
random seed to control the uncertainty associated with the stochasticity.

#### {.solution}
```{r}
# Type your solution here...
library(randomForest)

# Set a random seed for reproducibility
set.seed(857)

X <- Boston[, -ncol(Boston)]  # 14 Features (all but the last column)
y <- Boston$medv              # Target variable (last column)

# Range of values for mtry and ntree
mtry_values <- c(2, 4, 6, 8, 10, 12)    
ntree_values <- c(200, 300, 400, 500, 600)  

# Empty matrix or list to store the results
results <- matrix(NA, nrow = length(mtry_values), ncol = length(ntree_values))
colnames(results) <- paste0("ntree", ntree_values)
rownames(results) <- paste0("mtry", mtry_values)

# Loop over all mtry and ntree values
for (m in 1:length(mtry_values)) {
  for (n in 1:length(ntree_values)) {
    
    # Vector to store MSE for each run 
    oob_mse <- numeric(5)  
    
    # Repeat the Random Forest training 5 times for each combination
    for (i in 1:5) {
      
      # Train the Random Forest model with the specified mtry and ntree
      rf_model <- randomForest(x = X, y = y, 
                               mtry = mtry_values[m], 
                               ntree = ntree_values[n],
                               importance = TRUE)
      
      # Extract the out-of-bag MSE from the model
      oob_mse[i] <- tail(rf_model$mse, 1)  # Use the last OOB MSE value for the final number of trees
    }
    
    # Store the average OOB MSE for this combination
    results[m, n] <- mean(oob_mse)
  }
}

# Best mtry and ntree combination based on the lowest OOB MSE
best_params <- which(results == min(results), arr.ind = TRUE)
best_mtry <- mtry_values[best_params[1]]
best_ntree <- ntree_values[best_params[2]]

# Print the results
cat("Best mtry:", best_mtry, "\nBest ntree:", best_ntree, "\n")

# Graphical representation
library(reshape2)
results_melted <- melt(results)
colnames(results_melted) <- c("mtry", "ntree", "OOB_MSE")

# Use ggplot2 for plotting
library(ggplot2)
ggplot(results_melted, aes(x = as.factor(ntree), y = OOB_MSE, color = as.factor(mtry), group = mtry)) +
  geom_line() +
  geom_point() +
  labs(title = "Average OOB MSE vs ntree for different mtry values",
       x = "Number of Trees (ntree)",
       y = "Average out-of-bag MSE",
       color = "mtry")
```
Write your conclusion here...
